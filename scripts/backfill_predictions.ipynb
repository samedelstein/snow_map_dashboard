{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031c7a12-2ac0-423b-a638-eac3e05d9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Backfill historical predictions from snapshots.csv without leakage.\n",
    "\n",
    "Output:\n",
    "  data/artifacts_snow/predictions_backfill_prob.csv\n",
    "\n",
    "What it does:\n",
    "- Loads snapshots (local or remote)\n",
    "- Builds labels from lastserviced changes (next service time)\n",
    "- Builds storm envelopes from NWS alerts log and refines operational start/end using observed service activity\n",
    "- Adds features (including storm clock + 15m tempo + phase + route completion)\n",
    "- For each eventid:\n",
    "    - For a set of \"as_of\" cutpoints, train on data <= cutpoint\n",
    "    - Predict probabilities on the next time window\n",
    "- Enforces \"point-in-time\" correctness:\n",
    "    - Keep only rows with as_of_cut_ts < snapshot_ts\n",
    "    - For each (eventid, segment, snapshot_ts), keep the latest as_of_cut_ts BEFORE snapshot_ts\n",
    "\n",
    "Saves per-(eventid, segment, snapshot_ts):\n",
    "  p_1h, p_2h, p_4h, p_8h\n",
    "  plus labels and key metadata.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# -----------------------------\n",
    "# Constants / paths\n",
    "# -----------------------------\n",
    "EVENT = \"eventid\"\n",
    "SEG = \"snowroutesegmentid\"\n",
    "TS = \"snapshot_ts\"\n",
    "HORIZONS = [1, 2, 4, 8]\n",
    "\n",
    "SNAPSHOT_URL = (\n",
    "    \"https://raw.githubusercontent.com/\"\n",
    "    \"samedelstein/snow_map_dashboard/main/\"\n",
    "    \"data/snapshot_snow_routes/snapshots.csv\"\n",
    ")\n",
    "\n",
    "NWS_ALERTS_LOG_REMOTE = (\n",
    "    \"https://raw.githubusercontent.com/samedelstein/snow_map_dashboard/main/\"\n",
    "    \"data/artifacts_snow/nws_alerts_log.csv\"\n",
    ")\n",
    "\n",
    "CALIBRATION_MIN_ROWS_ISOTONIC = 500\n",
    "\n",
    "def get_repo_root() -> Path:\n",
    "    if \"__file__\" in globals():\n",
    "        return Path(__file__).resolve().parents[1]\n",
    "    return Path.cwd().resolve()\n",
    "\n",
    "REPO_ROOT = get_repo_root()\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "ARTIFACT_DIR = DATA_DIR / \"artifacts_snow\"\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NWS_ALERTS_LOG_LOCAL = str(ARTIFACT_DIR / \"nws_alerts_log.csv\")\n",
    "\n",
    "# storm envelope params (match your production)\n",
    "SNOW_ALERT_EVENTS = {\n",
    "    \"Winter Storm Warning\",\n",
    "    \"Winter Storm Watch\",\n",
    "    \"Winter Weather Advisory\",\n",
    "}\n",
    "ALERT_START_PAD_H = 6\n",
    "ALERT_END_PAD_H = 24\n",
    "\n",
    "# operational detection params (match production, tune if needed)\n",
    "OPS_BUCKET_MIN = \"15min\"\n",
    "OPS_MIN_SERVICES_PER_BUCKET = 10\n",
    "OPS_SUSTAIN_BUCKETS = 2\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Calibrator helper (fallback)\n",
    "# -----------------------------\n",
    "class _PrefitCalibrator:\n",
    "    def __init__(self, estimator: HistGradientBoostingClassifier, method: str) -> None:\n",
    "        self.estimator = estimator\n",
    "        self.method = method\n",
    "        self.calibrator: IsotonicRegression | LogisticRegression | None = None\n",
    "        self.classes_ = np.array([0, 1])\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> \"_PrefitCalibrator\":\n",
    "        p = self.estimator.predict_proba(X)[:, 1]\n",
    "        if self.method == \"isotonic\":\n",
    "            self.calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "            self.calibrator.fit(p, y)\n",
    "        else:\n",
    "            self.calibrator = LogisticRegression(solver=\"lbfgs\")\n",
    "            self.calibrator.fit(p.reshape(-1, 1), y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        if self.calibrator is None:\n",
    "            raise ValueError(\"Calibrator not fitted.\")\n",
    "        p = self.estimator.predict_proba(X)[:, 1]\n",
    "        if isinstance(self.calibrator, IsotonicRegression):\n",
    "            p_cal = self.calibrator.predict(p)\n",
    "        else:\n",
    "            p_cal = self.calibrator.predict_proba(p.reshape(-1, 1))[:, 1]\n",
    "        p_cal = np.clip(p_cal, 0.0, 1.0)\n",
    "        return np.column_stack([1 - p_cal, p_cal])\n",
    "\n",
    "\n",
    "def _fit_prefit_calibrator(\n",
    "    estimator: HistGradientBoostingClassifier,\n",
    "    X_calib: pd.DataFrame,\n",
    "    y_calib: pd.Series,\n",
    "    method: str,\n",
    "):\n",
    "    try:\n",
    "        cal = CalibratedClassifierCV(estimator, cv=\"prefit\", method=method)\n",
    "        cal.fit(X_calib, y_calib)\n",
    "        return cal\n",
    "    except Exception:\n",
    "        cal = _PrefitCalibrator(estimator, method)\n",
    "        cal.fit(X_calib, y_calib)\n",
    "        return cal\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Alerts -> storm envelopes -> operational windows\n",
    "# -----------------------------\n",
    "def load_alert_log(source: str | None = None) -> pd.DataFrame:\n",
    "    src = source or (NWS_ALERTS_LOG_LOCAL if os.path.exists(NWS_ALERTS_LOG_LOCAL) else NWS_ALERTS_LOG_REMOTE)\n",
    "    try:\n",
    "        a = pd.read_csv(src)\n",
    "    except Exception:\n",
    "        return pd.DataFrame(columns=[\"event\", \"start_ts\", \"end_ts\", \"severity\"])\n",
    "\n",
    "    if a.empty:\n",
    "        return pd.DataFrame(columns=[\"event\", \"start_ts\", \"end_ts\", \"severity\"])\n",
    "\n",
    "    a[\"start_ts\"] = pd.to_datetime(a[\"start_ts\"], utc=True, errors=\"coerce\")\n",
    "    a[\"end_ts\"] = pd.to_datetime(a[\"end_ts\"], utc=True, errors=\"coerce\")\n",
    "    a[\"event\"] = a[\"event\"].astype(str)\n",
    "    a[\"severity\"] = a.get(\"severity\", \"\").astype(str)\n",
    "    a = a[a[\"start_ts\"].notna() & a[\"end_ts\"].notna()].copy()\n",
    "    return a\n",
    "\n",
    "\n",
    "def build_storm_envelopes_from_alerts(alerts: pd.DataFrame) -> pd.DataFrame:\n",
    "    if alerts.empty:\n",
    "        return pd.DataFrame(columns=[\"storm_id\", \"storm_envelope_start\", \"storm_envelope_end\", \"severity_max\"])\n",
    "\n",
    "    a = alerts[alerts[\"event\"].isin(SNOW_ALERT_EVENTS)].copy()\n",
    "    if a.empty:\n",
    "        return pd.DataFrame(columns=[\"storm_id\", \"storm_envelope_start\", \"storm_envelope_end\", \"severity_max\"])\n",
    "\n",
    "    a[\"storm_envelope_start\"] = a[\"start_ts\"] - pd.Timedelta(hours=ALERT_START_PAD_H)\n",
    "    a[\"storm_envelope_end\"] = a[\"end_ts\"] + pd.Timedelta(hours=ALERT_END_PAD_H)\n",
    "\n",
    "    sev_rank = {\"Minor\": 1, \"Moderate\": 2, \"Severe\": 3, \"Extreme\": 4}\n",
    "    a[\"sev_rank\"] = a[\"severity\"].map(sev_rank).fillna(0).astype(int)\n",
    "\n",
    "    a = a.sort_values(\"storm_envelope_start\").reset_index(drop=True)\n",
    "\n",
    "    merged: list[tuple[pd.Timestamp, pd.Timestamp, int]] = []\n",
    "    cur_start = None\n",
    "    cur_end = None\n",
    "    cur_sev = 0\n",
    "\n",
    "    for row in a.itertuples(index=False):\n",
    "        s = row.storm_envelope_start\n",
    "        e = row.storm_envelope_end\n",
    "        sev = int(row.sev_rank)\n",
    "\n",
    "        if cur_start is None:\n",
    "            cur_start, cur_end, cur_sev = s, e, sev\n",
    "            continue\n",
    "\n",
    "        if s <= cur_end:\n",
    "            cur_end = max(cur_end, e)\n",
    "            cur_sev = max(cur_sev, sev)\n",
    "        else:\n",
    "            merged.append((cur_start, cur_end, cur_sev))\n",
    "            cur_start, cur_end, cur_sev = s, e, sev\n",
    "\n",
    "    if cur_start is not None:\n",
    "        merged.append((cur_start, cur_end, cur_sev))\n",
    "\n",
    "    inv_sev = {v: k for k, v in sev_rank.items()}\n",
    "    out = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"storm_id\": f\"storm_{i+1:03d}\",\n",
    "                \"storm_envelope_start\": s,\n",
    "                \"storm_envelope_end\": e,\n",
    "                \"severity_max\": inv_sev.get(sev, \"Unknown\"),\n",
    "            }\n",
    "            for i, (s, e, sev) in enumerate(merged)\n",
    "        ]\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def derive_city_service_events(labeled: pd.DataFrame) -> pd.DataFrame:\n",
    "    if labeled.empty:\n",
    "        return pd.DataFrame(columns=[EVENT, TS])\n",
    "\n",
    "    tmp = labeled.sort_values([EVENT, SEG, TS]).copy()\n",
    "    tmp[\"prev_last\"] = tmp.groupby([EVENT, SEG])[\"lastserviced\"].shift(1)\n",
    "\n",
    "    # IMPORTANT: prevent first-row NaT/NaT from becoming a \"change\"\n",
    "    tmp[\"lastserviced_changed\"] = (\n",
    "        tmp[\"lastserviced\"].notna()\n",
    "        & tmp[\"prev_last\"].notna()\n",
    "        & (tmp[\"lastserviced\"] != tmp[\"prev_last\"])\n",
    "    )\n",
    "\n",
    "    svc = tmp.loc[tmp[\"lastserviced_changed\"], [EVENT, TS]].copy()\n",
    "    return svc\n",
    "\n",
    "\n",
    "def refine_operational_windows(storms: pd.DataFrame, service_events: pd.DataFrame) -> pd.DataFrame:\n",
    "    if storms.empty:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"storm_id\",\"storm_envelope_start\",\"storm_envelope_end\",\n",
    "            \"storm_operational_start\",\"storm_operational_end\",\"severity_max\"\n",
    "        ])\n",
    "\n",
    "    if service_events.empty:\n",
    "        out = storms.copy()\n",
    "        out[\"storm_operational_start\"] = pd.NaT\n",
    "        out[\"storm_operational_end\"] = pd.NaT\n",
    "        return out\n",
    "\n",
    "    svc = service_events.copy()\n",
    "    svc[\"bucket\"] = svc[TS].dt.floor(OPS_BUCKET_MIN)\n",
    "\n",
    "    out_rows = []\n",
    "    for st in storms.itertuples(index=False):\n",
    "        s0 = st.storm_envelope_start\n",
    "        s1 = st.storm_envelope_end\n",
    "\n",
    "        ssvc = svc[(svc[TS] >= s0) & (svc[TS] <= s1)]\n",
    "        if ssvc.empty:\n",
    "            out_rows.append({\n",
    "                \"storm_id\": st.storm_id,\n",
    "                \"storm_envelope_start\": s0,\n",
    "                \"storm_envelope_end\": s1,\n",
    "                \"storm_operational_start\": pd.NaT,\n",
    "                \"storm_operational_end\": pd.NaT,\n",
    "                \"severity_max\": st.severity_max,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        counts = (\n",
    "            ssvc.groupby(\"bucket\")\n",
    "                .size()\n",
    "                .rename(\"services\")\n",
    "                .reset_index()\n",
    "                .sort_values(\"bucket\")\n",
    "        )\n",
    "\n",
    "        active = counts[\"services\"] >= OPS_MIN_SERVICES_PER_BUCKET\n",
    "\n",
    "        op_start = pd.NaT\n",
    "        op_end = pd.NaT\n",
    "        if active.any():\n",
    "            sustain = (\n",
    "                active.rolling(OPS_SUSTAIN_BUCKETS, min_periods=OPS_SUSTAIN_BUCKETS).sum()\n",
    "                >= OPS_SUSTAIN_BUCKETS\n",
    "            )\n",
    "            if sustain.any():\n",
    "                first_idx = int(np.argmax(sustain.to_numpy()))\n",
    "                op_start = counts.iloc[first_idx][\"bucket\"]\n",
    "                last_active_idx = int(np.where(active.to_numpy())[0].max())\n",
    "                op_end = counts.iloc[last_active_idx][\"bucket\"] + pd.Timedelta(OPS_BUCKET_MIN)\n",
    "\n",
    "        out_rows.append({\n",
    "            \"storm_id\": st.storm_id,\n",
    "            \"storm_envelope_start\": s0,\n",
    "            \"storm_envelope_end\": s1,\n",
    "            \"storm_operational_start\": op_start,\n",
    "            \"storm_operational_end\": op_end,\n",
    "            \"severity_max\": st.severity_max,\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(out_rows)\n",
    "    out[\"storm_operational_start\"] = pd.to_datetime(out[\"storm_operational_start\"], utc=True, errors=\"coerce\")\n",
    "    out[\"storm_operational_end\"] = pd.to_datetime(out[\"storm_operational_end\"], utc=True, errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def attach_storm_context(df: pd.DataFrame, storms_ops: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"storm_id\"] = \"no_storm\"\n",
    "    # force dtype upfront\n",
    "    out[\"storm_operational_start\"] = pd.to_datetime(pd.Series([pd.NaT] * len(out)), utc=True)\n",
    "    out[\"storm_operational_end\"] = pd.to_datetime(pd.Series([pd.NaT] * len(out)), utc=True)\n",
    "    out[\"storm_severity_max\"] = \"Unknown\"\n",
    "\n",
    "    if out.empty or storms_ops.empty:\n",
    "        return out\n",
    "\n",
    "    storms = storms_ops.copy()\n",
    "    storms[\"storm_operational_start\"] = pd.to_datetime(storms[\"storm_operational_start\"], utc=True, errors=\"coerce\")\n",
    "    storms[\"storm_operational_end\"] = pd.to_datetime(storms[\"storm_operational_end\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "    for st in storms.itertuples(index=False):\n",
    "        mask = (out[TS] >= st.storm_envelope_start) & (out[TS] <= st.storm_envelope_end)\n",
    "        out.loc[mask, \"storm_id\"] = st.storm_id\n",
    "        out.loc[mask, \"storm_operational_start\"] = st.storm_operational_start\n",
    "        out.loc[mask, \"storm_operational_end\"] = st.storm_operational_end\n",
    "        out.loc[mask, \"storm_severity_max\"] = st.severity_max\n",
    "\n",
    "    out[\"storm_operational_start\"] = pd.to_datetime(out[\"storm_operational_start\"], utc=True, errors=\"coerce\")\n",
    "    out[\"storm_operational_end\"] = pd.to_datetime(out[\"storm_operational_end\"], utc=True, errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Snapshots -> labels\n",
    "# -----------------------------\n",
    "def load_snapshots(path_or_url: str | Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path_or_url)\n",
    "    df[TS] = pd.to_datetime(df[TS], utc=True, errors=\"coerce\")\n",
    "\n",
    "    for c in [\"lastserviced\", \"lastserviceleft\", \"lastserviceright\"]:\n",
    "        if c in df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df[c]):\n",
    "                df[c] = pd.to_datetime(df[c], unit=\"ms\", utc=True, errors=\"coerce\")\n",
    "            else:\n",
    "                df[c] = pd.to_datetime(df[c], utc=True, errors=\"coerce\")\n",
    "\n",
    "    df[\"routepriority\"] = df.get(\"routepriority\", \"Unknown\").fillna(\"Unknown\").astype(str)\n",
    "    df[\"snowrouteid\"] = df.get(\"snowrouteid\", \"Unknown\").fillna(\"Unknown\").astype(str)\n",
    "    df[\"roadname\"] = df.get(\"roadname\", \"Unknown\").fillna(\"Unknown\").astype(str)\n",
    "\n",
    "    for c in [\"passes\", \"passesleft\", \"passesright\"]:\n",
    "        df[c] = pd.to_numeric(df.get(c, 0), errors=\"coerce\").fillna(0)\n",
    "\n",
    "    df[\"segmentlength\"] = pd.to_numeric(df.get(\"segmentlength\"), errors=\"coerce\")\n",
    "\n",
    "    df = df[df[TS].notna() & df[EVENT].notna() & df[SEG].notna()].copy()\n",
    "    df[EVENT] = df[EVENT].astype(str)\n",
    "    df[SEG] = df[SEG].astype(str)\n",
    "\n",
    "    if \"passes_phase\" in df.columns:\n",
    "        df[\"passes_event\"] = df[\"passes_phase\"].fillna(df[\"passes\"])\n",
    "    else:\n",
    "        df[\"passes_event\"] = df[\"passes\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_events(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    s = df.sort_values([EVENT, SEG, TS]).copy()\n",
    "    s[\"prev_last\"] = s.groupby([EVENT, SEG])[\"lastserviced\"].shift(1)\n",
    "    ev = s[(s[\"lastserviced\"].notna()) & (s[\"lastserviced\"] != s[\"prev_last\"])][\n",
    "        [EVENT, SEG, TS, \"lastserviced\"]\n",
    "    ].copy()\n",
    "    ev = ev.rename(columns={TS: \"observed_at\", \"lastserviced\": \"serviced_at\"})\n",
    "    return ev.sort_values([EVENT, SEG, \"serviced_at\"])\n",
    "\n",
    "\n",
    "def label_next_service(df: pd.DataFrame, events: pd.DataFrame) -> pd.DataFrame:\n",
    "    svc_times = {(e, s): g[\"serviced_at\"].values for (e, s), g in events.groupby([EVENT, SEG])}\n",
    "\n",
    "    labeled = df.sort_values([EVENT, SEG, TS]).copy()\n",
    "    next_times = []\n",
    "\n",
    "    for row in labeled[[EVENT, SEG, TS]].itertuples(index=False):\n",
    "        times = svc_times.get((row.eventid, row.snowroutesegmentid))\n",
    "        if times is None or len(times) == 0:\n",
    "            next_times.append(pd.NaT)\n",
    "            continue\n",
    "\n",
    "        times64 = pd.to_datetime(times, utc=True).to_numpy(dtype=\"datetime64[ns]\")\n",
    "        idx = np.searchsorted(times64, row.snapshot_ts.to_datetime64(), side=\"right\")\n",
    "        next_times.append(pd.to_datetime(times64[idx], utc=True) if idx < len(times64) else pd.NaT)\n",
    "\n",
    "    labeled[\"next_serviced_at\"] = pd.to_datetime(next_times, utc=True)\n",
    "    labeled[\"hours_to_next_service\"] = (labeled[\"next_serviced_at\"] - labeled[TS]).dt.total_seconds() / 3600.0\n",
    "    labeled[\"censored\"] = labeled[\"hours_to_next_service\"].isna()\n",
    "    return labeled\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Feature engineering (MATCHES new production signals)\n",
    "# -----------------------------\n",
    "def mark_untracked(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    growth = out.groupby([EVENT, SEG])[\"passes_event\"].agg([\"min\", \"max\"]).reset_index()\n",
    "    growth[\"growth\"] = growth[\"max\"] - growth[\"min\"]\n",
    "    out = out.merge(growth[[EVENT, SEG, \"growth\"]], on=[EVENT, SEG], how=\"left\")\n",
    "\n",
    "    out[\"prediction_status\"] = \"OK\"\n",
    "    out.loc[(out[\"growth\"] == 0) | (out[\"snowrouteid\"].str.lower() == \"unknown\"), \"prediction_status\"] = \"NO_PRED_UNTRACKED\"\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_horizon_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for h in HORIZONS:\n",
    "        out[f\"y_{h}h\"] = (\n",
    "            out[\"hours_to_next_service\"].notna()\n",
    "            & (out[\"hours_to_next_service\"] <= h)\n",
    "        ).astype(int)\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_features(df: pd.DataFrame, events: pd.DataFrame) -> pd.DataFrame:\n",
    "    f = df.sort_values([EVENT, SEG, TS]).copy()\n",
    "\n",
    "    # ensure storm columns exist (defensive)\n",
    "    for c, default in [\n",
    "        (\"storm_id\", \"no_storm\"),\n",
    "        (\"storm_operational_start\", pd.NaT),\n",
    "        (\"storm_operational_end\", pd.NaT),\n",
    "        (\"storm_severity_max\", \"Unknown\"),\n",
    "    ]:\n",
    "        if c not in f.columns:\n",
    "            f[c] = default\n",
    "    f[\"storm_operational_start\"] = pd.to_datetime(f[\"storm_operational_start\"], utc=True, errors=\"coerce\")\n",
    "    f[\"storm_operational_end\"] = pd.to_datetime(f[\"storm_operational_end\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "    # base features\n",
    "    f[\"priority_num\"] = f[\"routepriority\"].str.extract(r\"(\\d+)\").astype(float)\n",
    "    f[\"hour\"] = f[TS].dt.hour\n",
    "    f[\"dow\"] = f[TS].dt.weekday\n",
    "\n",
    "    # hours since last service (segment-level)\n",
    "    f[\"hours_since_last_service\"] = (f[TS] - f[\"lastserviced\"]).dt.total_seconds() / 3600.0\n",
    "\n",
    "    denom = f[\"segmentlength\"].replace(0, np.nan)\n",
    "    f[\"passes_per_len\"] = f[\"passes_event\"] / denom\n",
    "\n",
    "    # last serviced change (robust)\n",
    "    f[\"prev_last\"] = f.groupby([EVENT, SEG])[\"lastserviced\"].shift(1)\n",
    "    f[\"lastserviced_changed\"] = (\n",
    "        f[\"lastserviced\"].notna()\n",
    "        & f[\"prev_last\"].notna()\n",
    "        & (f[\"lastserviced\"] != f[\"prev_last\"])\n",
    "    ).astype(int)\n",
    "\n",
    "    # buckets\n",
    "    f[\"hour_bucket\"] = f[TS].dt.floor(\"h\")\n",
    "    f[\"bucket_15m\"] = f[TS].dt.floor(\"15min\")\n",
    "\n",
    "    # phase (if present)\n",
    "    if \"eventphaseid\" in f.columns:\n",
    "        phase_num = f[\"eventphaseid\"].astype(str).str.extract(r\"-(\\d+)$\")[0]\n",
    "        f[\"phase_num\"] = pd.to_numeric(phase_num, errors=\"coerce\").fillna(0)\n",
    "    else:\n",
    "        f[\"phase_num\"] = 0\n",
    "\n",
    "    # storm clock\n",
    "    f[\"in_storm\"] = (\n",
    "        f[\"storm_operational_start\"].notna()\n",
    "        & f[\"storm_operational_end\"].notna()\n",
    "        & (f[TS] >= f[\"storm_operational_start\"])\n",
    "        & (f[TS] <= f[\"storm_operational_end\"])\n",
    "    ).astype(int)\n",
    "\n",
    "    f[\"hours_since_storm_start\"] = ((f[TS] - f[\"storm_operational_start\"]).dt.total_seconds() / 3600.0)\n",
    "    f[\"hours_until_storm_end\"] = ((f[\"storm_operational_end\"] - f[TS]).dt.total_seconds() / 3600.0)\n",
    "    f[\"hours_since_storm_start\"] = f[\"hours_since_storm_start\"].clip(lower=0).fillna(-1)\n",
    "    f[\"hours_until_storm_end\"] = f[\"hours_until_storm_end\"].clip(lower=0).fillna(-1)\n",
    "\n",
    "    # city and route tempo (15m)\n",
    "    svc = f.loc[f[\"lastserviced_changed\"] == 1, [EVENT, \"snowrouteid\", TS]].copy()\n",
    "    svc[\"bucket_15m\"] = svc[TS].dt.floor(\"15min\")\n",
    "\n",
    "    city_15m = svc.groupby([EVENT, \"bucket_15m\"]).size().rename(\"city_services_15m\").reset_index()\n",
    "    f = f.merge(city_15m, on=[EVENT, \"bucket_15m\"], how=\"left\")\n",
    "    f[\"city_services_15m\"] = f[\"city_services_15m\"].fillna(0)\n",
    "\n",
    "    route_15m = svc.groupby([EVENT, \"snowrouteid\", \"bucket_15m\"]).size().rename(\"route_services_15m\").reset_index()\n",
    "    f = f.merge(route_15m, on=[EVENT, \"snowrouteid\", \"bucket_15m\"], how=\"left\")\n",
    "    f[\"route_services_15m\"] = f[\"route_services_15m\"].fillna(0)\n",
    "\n",
    "    # city services last hour (from events)\n",
    "    city = (\n",
    "        events.assign(hour_bucket=events[\"serviced_at\"].dt.floor(\"h\"))\n",
    "        .groupby([EVENT, \"hour_bucket\"])\n",
    "        .size()\n",
    "        .rename(\"city_services_last_hour\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    f = f.merge(city, on=[EVENT, \"hour_bucket\"], how=\"left\")\n",
    "    f[\"city_services_last_hour\"] = f[\"city_services_last_hour\"].fillna(0)\n",
    "\n",
    "    # route services last hour\n",
    "    route = (\n",
    "        f.groupby([EVENT, \"snowrouteid\", \"hour_bucket\"])[\"lastserviced_changed\"]\n",
    "        .sum()\n",
    "        .rename(\"route_services_last_hour\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    f = f.merge(route, on=[EVENT, \"snowrouteid\", \"hour_bucket\"], how=\"left\")\n",
    "    f[\"route_services_last_hour\"] = f[\"route_services_last_hour\"].fillna(0)\n",
    "\n",
    "    # route completion (60m)\n",
    "    f[\"_route_served_once\"] = f.groupby([EVENT, \"snowrouteid\", SEG])[\"lastserviced_changed\"].cummax()\n",
    "    route_completion = (\n",
    "        f.groupby([EVENT, \"snowrouteid\", \"hour_bucket\"])[\"_route_served_once\"]\n",
    "        .mean()\n",
    "        .rename(\"route_completion_60m\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    f = f.merge(route_completion, on=[EVENT, \"snowrouteid\", \"hour_bucket\"], how=\"left\")\n",
    "    f[\"route_completion_60m\"] = f[\"route_completion_60m\"].fillna(0)\n",
    "    f = f.drop(columns=[\"_route_served_once\"])\n",
    "\n",
    "    # rolling 3h/6h service counts + passes deltas/ratios\n",
    "    def rolling_sum_by_group(data, group_cols, value_col, window_h):\n",
    "        ordered = data.sort_values(group_cols + [\"hour_bucket\"]).copy()\n",
    "        rolled = (\n",
    "            ordered.set_index(\"hour_bucket\")\n",
    "            .groupby(group_cols, sort=False)[value_col]\n",
    "            .rolling(f\"{window_h}h\", min_periods=1)\n",
    "            .sum()\n",
    "            .reset_index(level=group_cols, drop=True)\n",
    "        )\n",
    "        rolled.index = ordered.index\n",
    "        return rolled.reindex(data.index)\n",
    "\n",
    "    def rolling_delta_ratio_by_group(data, group_cols, value_col, window_h, prefix):\n",
    "        ordered = data.sort_values(group_cols + [\"hour_bucket\"]).copy()\n",
    "        series = ordered.set_index(\"hour_bucket\").groupby(group_cols, sort=False)[value_col]\n",
    "        ro = series.rolling(f\"{window_h}h\", min_periods=1)\n",
    "        rmin = ro.min().reset_index(level=group_cols, drop=True)\n",
    "        rmax = ro.max().reset_index(level=group_cols, drop=True)\n",
    "        delta = (rmax - rmin).to_numpy()\n",
    "        ratio = (rmax / rmin.replace(0, np.nan)).to_numpy()\n",
    "        out = pd.DataFrame(\n",
    "            {\n",
    "                f\"{prefix}_delta_{window_h}h\": delta,\n",
    "                f\"{prefix}_ratio_{window_h}h\": ratio,\n",
    "            },\n",
    "            index=ordered.index,\n",
    "        )\n",
    "        return out.reindex(data.index)\n",
    "\n",
    "    for w in [3, 6]:\n",
    "        f[f\"seg_services_{w}h\"] = rolling_sum_by_group(\n",
    "            f, [EVENT, SEG], \"lastserviced_changed\", w\n",
    "        ).fillna(0)\n",
    "        f[f\"route_services_{w}h\"] = rolling_sum_by_group(\n",
    "            f, [EVENT, \"snowrouteid\"], \"lastserviced_changed\", w\n",
    "        ).fillna(0)\n",
    "        f = f.join(rolling_delta_ratio_by_group(f, [EVENT, SEG], \"passes_event\", w, \"passes_event\").fillna(0))\n",
    "        f = f.join(rolling_delta_ratio_by_group(f, [EVENT, \"snowrouteid\"], \"passes_event\", w, \"route_passes_event\").fillna(0))\n",
    "\n",
    "    # placeholders (keep consistent with earlier backfill)\n",
    "    f[\"neighbor_services_last_hour\"] = 0\n",
    "    f[\"temp_c\"] = np.nan\n",
    "    f[\"wind_speed_mps\"] = np.nan\n",
    "    f[\"wind_gust_mps\"] = np.nan\n",
    "    f[\"snowfall_rate_mmhr\"] = np.nan\n",
    "    f[\"freezing_rain\"] = 0\n",
    "    for lag in [1, 2, 3]:\n",
    "        f[f\"temp_c_lag{lag}\"] = np.nan\n",
    "        f[f\"snowfall_rate_mmhr_lag{lag}\"] = np.nan\n",
    "        f[f\"wind_speed_mps_lag{lag}\"] = np.nan\n",
    "        f[f\"wind_gust_mps_lag{lag}\"] = np.nan\n",
    "    f[\"nws_alert_count\"] = 0\n",
    "    f[\"nws_alert_active\"] = 0\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"priority_num\",\n",
    "    \"passes_event\",\n",
    "    \"passes_per_len\",\n",
    "    \"hours_since_last_service\",\n",
    "    \"hour\",\n",
    "    \"dow\",\n",
    "    \"city_services_last_hour\",\n",
    "    \"route_services_last_hour\",\n",
    "    \"seg_services_3h\",\n",
    "    \"seg_services_6h\",\n",
    "    \"route_services_3h\",\n",
    "    \"route_services_6h\",\n",
    "    \"passes_event_delta_3h\",\n",
    "    \"passes_event_ratio_3h\",\n",
    "    \"passes_event_delta_6h\",\n",
    "    \"passes_event_ratio_6h\",\n",
    "    \"route_passes_event_delta_3h\",\n",
    "    \"route_passes_event_ratio_3h\",\n",
    "    \"route_passes_event_delta_6h\",\n",
    "    \"route_passes_event_ratio_6h\",\n",
    "    \"neighbor_services_last_hour\",\n",
    "    \"temp_c\",\n",
    "    \"wind_speed_mps\",\n",
    "    \"wind_gust_mps\",\n",
    "    \"snowfall_rate_mmhr\",\n",
    "    \"freezing_rain\",\n",
    "    \"temp_c_lag1\",\n",
    "    \"temp_c_lag2\",\n",
    "    \"temp_c_lag3\",\n",
    "    \"snowfall_rate_mmhr_lag1\",\n",
    "    \"snowfall_rate_mmhr_lag2\",\n",
    "    \"snowfall_rate_mmhr_lag3\",\n",
    "    \"wind_speed_mps_lag1\",\n",
    "    \"wind_speed_mps_lag2\",\n",
    "    \"wind_speed_mps_lag3\",\n",
    "    \"wind_gust_mps_lag1\",\n",
    "    \"wind_gust_mps_lag2\",\n",
    "    \"wind_gust_mps_lag3\",\n",
    "    \"nws_alert_count\",\n",
    "    \"nws_alert_active\",\n",
    "\n",
    "    # NEW signals (match snow_predict.py)\n",
    "    \"phase_num\",\n",
    "    \"in_storm\",\n",
    "    \"hours_since_storm_start\",\n",
    "    \"hours_until_storm_end\",\n",
    "    \"city_services_15m\",\n",
    "    \"route_services_15m\",\n",
    "    \"route_completion_60m\",\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Modeling\n",
    "# -----------------------------\n",
    "def train_models_point_in_time(train_df: pd.DataFrame):\n",
    "    train_cutoffs = train_df.groupby(EVENT)[TS].transform(lambda s: s.quantile(0.8))\n",
    "    train_mask = train_df[TS] <= train_cutoffs\n",
    "\n",
    "    X = train_df[FEATURE_COLS].replace([np.inf, -np.inf], np.nan)\n",
    "    med = X.median(numeric_only=True)\n",
    "    X = X.fillna(med)\n",
    "\n",
    "    calib_cutoffs = train_df.loc[train_mask].groupby(EVENT)[TS].transform(lambda s: s.quantile(0.8))\n",
    "    calib_mask = pd.Series(False, index=train_df.index)\n",
    "    calib_mask.loc[train_mask] = train_df.loc[train_mask, TS] > calib_cutoffs\n",
    "    base_train_mask = train_mask & ~calib_mask\n",
    "\n",
    "    models: dict[int, HistGradientBoostingClassifier | None] = {}\n",
    "    calibrated: dict[int, CalibratedClassifierCV | _PrefitCalibrator | None] = {}\n",
    "\n",
    "    for h in HORIZONS:\n",
    "        y = train_df[f\"y_{h}h\"].astype(int)\n",
    "        X_train, y_train = X[base_train_mask], y[base_train_mask]\n",
    "        X_cal, y_cal = X[calib_mask], y[calib_mask]\n",
    "\n",
    "        if X_train.empty or y_train.nunique() < 2:\n",
    "            models[h] = None\n",
    "            calibrated[h] = None\n",
    "            continue\n",
    "\n",
    "        pos = int(y_train.sum())\n",
    "        neg = int(len(y_train) - pos)\n",
    "        pos_rate = float(pos / len(y_train)) if len(y_train) else 0.0\n",
    "        if pos > 0 and neg > 0 and (pos_rate < 0.2 or pos_rate > 0.8):\n",
    "            pos_weight = neg / pos\n",
    "            sample_weight = np.where(y_train == 1, pos_weight, 1.0)\n",
    "        else:\n",
    "            sample_weight = None\n",
    "\n",
    "        clf = HistGradientBoostingClassifier(\n",
    "            max_depth=6, learning_rate=0.08, max_iter=350, random_state=42\n",
    "        )\n",
    "        clf.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        models[h] = clf\n",
    "\n",
    "        if not X_cal.empty and y_cal.nunique() >= 2:\n",
    "            method = \"isotonic\" if len(X_cal) >= CALIBRATION_MIN_ROWS_ISOTONIC else \"sigmoid\"\n",
    "            calibrated[h] = _fit_prefit_calibrator(clf, X_cal, y_cal, method)\n",
    "        else:\n",
    "            calibrated[h] = None\n",
    "\n",
    "    return models, calibrated, med\n",
    "\n",
    "\n",
    "def predict_with_models(df: pd.DataFrame, models, calibrated, medians: pd.Series) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    X = out[FEATURE_COLS].replace([np.inf, -np.inf], np.nan)\n",
    "    X = X.fillna(medians)\n",
    "\n",
    "    for h in HORIZONS:\n",
    "        model = calibrated.get(h) or models.get(h)\n",
    "        if model is None:\n",
    "            out[f\"p_{h}h\"] = np.nan\n",
    "        else:\n",
    "            out[f\"p_{h}h\"] = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    out[\"p_2h\"] = out[[\"p_1h\", \"p_2h\"]].max(axis=1)\n",
    "    out[\"p_4h\"] = out[[\"p_2h\", \"p_4h\"]].max(axis=1)\n",
    "    out[\"p_8h\"] = out[[\"p_4h\", \"p_8h\"]].max(axis=1)\n",
    "\n",
    "    for h in HORIZONS:\n",
    "        out.loc[out[\"prediction_status\"] != \"OK\", f\"p_{h}h\"] = np.nan\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main backfill\n",
    "# -----------------------------\n",
    "def main() -> None:\n",
    "    df = load_snapshots(SNAPSHOT_URL)\n",
    "    if df.empty:\n",
    "        raise SystemExit(\"No snapshots loaded\")\n",
    "\n",
    "    events = build_events(df)\n",
    "    labeled = label_next_service(df, events)\n",
    "\n",
    "    # storms from alerts + operational activity\n",
    "    alerts_log = load_alert_log()\n",
    "    storms_env = build_storm_envelopes_from_alerts(alerts_log)\n",
    "    service_events = derive_city_service_events(labeled)\n",
    "    storms_ops = refine_operational_windows(storms_env, service_events)\n",
    "    labeled = attach_storm_context(labeled, storms_ops)\n",
    "\n",
    "    featured = add_features(labeled, events)\n",
    "    featured = mark_untracked(featured)\n",
    "    featured = add_horizon_labels(featured)\n",
    "\n",
    "    backfills = []\n",
    "    cut_every_hours = 6\n",
    "    predict_window_hours = 6\n",
    "\n",
    "    for event_id, ev_df in featured.groupby(EVENT):\n",
    "        ev_df = ev_df.sort_values(TS).copy()\n",
    "        times = ev_df[TS].dropna().sort_values().unique()\n",
    "        if len(times) < 50:\n",
    "            continue\n",
    "\n",
    "        t0 = pd.to_datetime(times[0], utc=True)\n",
    "        t1 = pd.to_datetime(times[-1], utc=True)\n",
    "        cutpoints = pd.date_range(start=t0, end=t1, freq=f\"{cut_every_hours}h\", tz=\"UTC\")\n",
    "\n",
    "        for cut in cutpoints:\n",
    "            train = ev_df[ev_df[TS] <= cut]\n",
    "            test = ev_df[(ev_df[TS] > cut) & (ev_df[TS] <= cut + pd.Timedelta(hours=predict_window_hours))]\n",
    "            if train.empty or test.empty:\n",
    "                continue\n",
    "\n",
    "            train_ok = train[train[\"prediction_status\"] == \"OK\"].copy()\n",
    "            test_ok = test[test[\"prediction_status\"] == \"OK\"].copy()\n",
    "            if len(train_ok) < 200 or len(test_ok) < 50:\n",
    "                continue\n",
    "\n",
    "            models, calibrated, med = train_models_point_in_time(train_ok)\n",
    "            pred = predict_with_models(test_ok, models, calibrated, med)\n",
    "\n",
    "            keep = [\n",
    "                EVENT, SEG, TS, \"snowrouteid\", \"routepriority\", \"priority_num\",\n",
    "                \"prediction_status\",\n",
    "                \"next_serviced_at\", \"hours_to_next_service\", \"censored\",\n",
    "                \"y_1h\", \"y_2h\", \"y_4h\", \"y_8h\",\n",
    "                \"p_1h\", \"p_2h\", \"p_4h\", \"p_8h\",\n",
    "            ]\n",
    "            pred[\"as_of_cut_ts\"] = cut\n",
    "            backfills.append(pred[keep + [\"as_of_cut_ts\"]])\n",
    "\n",
    "    if not backfills:\n",
    "        raise SystemExit(\"No backfill rows generated (try lowering thresholds / increasing windows).\")\n",
    "\n",
    "    out = pd.concat(backfills, ignore_index=True)\n",
    "\n",
    "    # --- CRITICAL: enforce correct point-in-time prediction (no leakage) ---\n",
    "    out[\"as_of_cut_ts\"] = pd.to_datetime(out[\"as_of_cut_ts\"], utc=True, errors=\"coerce\")\n",
    "    out[\"snapshot_ts\"] = pd.to_datetime(out[\"snapshot_ts\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "    out = out[out[\"as_of_cut_ts\"] < out[\"snapshot_ts\"]].copy()\n",
    "\n",
    "    out = (\n",
    "        out.sort_values([\"eventid\", \"snowroutesegmentid\", \"snapshot_ts\", \"as_of_cut_ts\"])\n",
    "           .drop_duplicates(subset=[\"eventid\", \"snowroutesegmentid\", \"snapshot_ts\"], keep=\"last\")\n",
    "           .sort_values([\"eventid\", \"snapshot_ts\", \"snowroutesegmentid\"])\n",
    "    )\n",
    "\n",
    "    out_path = ARTIFACT_DIR / \"predictions_backfill_prob.csv\"\n",
    "    out.to_csv(out_path, index=False)\n",
    "    print(f\"Saved backfill predictions: {out_path} | rows={len(out):,}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3337423c-4414-426c-bf46-538d1dd2bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(storms_ops[[\"storm_id\",\"storm_operational_start\",\"storm_operational_end\",\"severity_max\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed058512-0cf4-4caf-87c7-af266da4f05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c0aab-1115-46d6-91f2-3fbd0b0bcde5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
